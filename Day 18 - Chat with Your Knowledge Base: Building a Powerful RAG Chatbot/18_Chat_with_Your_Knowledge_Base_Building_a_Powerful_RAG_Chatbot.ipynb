{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "367451dc",
   "metadata": {},
   "source": [
    "# RAG Chatbot using reviews.csv\n",
    "This notebook builds a simple Retrieval-Augmented Generation (RAG) style chatbot over the `reviews.csv` dataset.\n",
    "\n",
    "It uses:\n",
    "- pandas for loading data\n",
    "- scikit-learn TF-IDF for embeddings\n",
    "- cosine similarity for retrieval\n",
    "- A simple response generator (can be replaced with an LLM later)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b08290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies (uncomment if running in Colab/Jupyter fresh environment)\n",
    "# !pip install pandas scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bfce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7321889",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8064c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to your uploaded file\n",
    "csv_path = \"reviews.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Number of rows:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca56ff",
   "metadata": {},
   "source": [
    "## Prepare Text Data\n",
    "We will combine relevant text columns into a single corpus for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a39b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Try to find a text column automatically\n",
    "text_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "\n",
    "print(\"Text columns detected:\", text_columns)\n",
    "\n",
    "# Choose the first text column by default\n",
    "text_col = text_columns[0]\n",
    "documents = df[text_col].astype(str).tolist()\n",
    "\n",
    "print(\"Using column:\", text_col)\n",
    "print(\"Sample document:\", documents[0][:200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc7dad",
   "metadata": {},
   "source": [
    "## Create Embeddings using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "doc_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "print(\"Document vectors shape:\", doc_vectors.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26c210",
   "metadata": {},
   "source": [
    "## Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85c7739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve(query, top_k=3):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, doc_vectors)[0]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            \"index\": int(idx),\n",
    "            \"score\": float(similarities[idx]),\n",
    "            \"text\": documents[idx]\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749eb726",
   "metadata": {},
   "source": [
    "## Simple Answer Generator\n",
    "This is a placeholder for an LLM. It uses retrieved documents to form a response.\n",
    "You can later replace this with OpenAI, Groq, or any LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_answer(query, retrieved_docs):\n",
    "    answer = f\"Question: {query}\\n\\nBased on the dataset, here are the most relevant entries:\\n\\n\"\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        answer += f\"{i}. (score={doc['score']:.3f}) {doc['text'][:300]}...\\n\\n\"\n",
    "    answer += \"You can plug this context into an LLM for a more natural answer.\"\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71731333",
   "metadata": {},
   "source": [
    "## Chat Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d96ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat(query, top_k=3):\n",
    "    retrieved = retrieve(query, top_k=top_k)\n",
    "    response = generate_answer(query, retrieved)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1494e3",
   "metadata": {},
   "source": [
    "## Try It Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbd62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What do people think about this product?\"\n",
    "print(chat(query, top_k=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaef5a7",
   "metadata": {},
   "source": [
    "## Interactive Loop (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while True:\n",
    "    q = input(\"Ask a question (or type 'exit'): \")\n",
    "    if q.lower() == \"exit\":\n",
    "        break\n",
    "    print(chat(q, top_k=3))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171819ab",
   "metadata": {},
   "source": [
    "## Next Improvements\n",
    "- Replace TF-IDF with sentence-transformer embeddings\n",
    "- Use FAISS or Chroma for vector search\n",
    "- Plug in an LLM (OpenAI / Groq / HuggingFace) for natural answers\n",
    "- Add a UI with Gradio or Streamlit"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}